---
title: "Codes & algorithms"
subtitle: "Técnicas de Estatística Computacional"
author: "Tailine J. S. Nonato"
date: today
date-format: long
format: html
---

:::: {.panel-tabset}

# Geração de NPA's


## Gerador congruencial linear (Lehmer)

### Passo a passo

1. Escolher um número inteiro positivo $x_0$ (semente) e um número inteiro positivo $a$ (multiplicador).

2. Escolher um número inteiro positivo $m$ (modulo) tal que $m > x_0$. Geralmente escolhe-se $m$ um número primo grande (Ex.: $2^{31}-1$).

3. Escolher um número inteiro positivo $a$ (incremento). Geralmente escolhe-se o valor clássico proposto por Park e Miller: $a = 16807$.

4. Gerar a sequência de números pseudo-aleatórios $x_i$ utilizando a seguinte fórmula:

$$
x_i = (a \cdot x_{i-1}) \mod m
$$

### No R

```{r}
#| label: "gerador-lehmer"

lehmer_generator <- function(seed, a, m, n) {
  x <- numeric(n)
  x[1] <- seed
  for (i in 2:n) {
    x[i] <- (a * x[i - 1]) %% m
  }
  return(x)
}

# Parâmetros clássicos 
seed <- 12345
a <- 16807         # valor clássico (proposto por Park e Miller)
m <- 2^31 - 1      # primo grande
n <- 100

# Gerar números
valores <- lehmer_generator(seed, a, m, n)

# Normalizar para intervalo [0,1]
valores_normalizados <- valores / m

# Ver primeiros valores
head(valores_normalizados)
```

## Método da Transformação Inversa

### Caso contínuo

$X \sim$ Exponencial $(1)$ com $f(x) = \lambda e^{-\lambda x}$, $x > 0$, $\lambda = 1$.

A função de distribuição acumulada é $F(x) = 1 - e^{-\lambda x}$, $x > 0$. E a inversa é $F^{-1}(u) = -\frac{1}{\lambda} \log(1-u)$, $u \in (0,1)$

```{r}
n <-200;lambda <- 1
u <- runif(n)
e <- -(log(1-u)/lambda) ## x <- -log(runif(n))/lambda
t <- seq(0,10,.01)
hist(e,probability = TRUE,main = "")
lines(t,lambda*exp(-lambda*(t)),col=c("red"))
```

### Caso discreto

$X \sim$ Geométrica $p(0)= 0.1, p(1) = 0.2, p(2) = 0.2, p(3) = 0.2, p(4) = 0.3$

```{r}
n <- 1000
p <- c(.1, .2, .2, .2, .3)
cdf <- cumsum(p)
x <- numeric(n)
for (i in 1:n) x[i] <- sum(as.integer(runif(1) > cdf))
rbind(table(x) / n, p)

## usando a função sample
n <- 1000
p <- c(.1, .2, .2, .2, .3)
x <- sample(0:4, size=n, prob=p, replace=TRUE)
rbind(table(x) / n, p)
```

## Método da Aceitação e Rejeição

$f$: $Beta(2.7,6.3)$ com $M = 2.7$

$g$: $Uniforme[0,1]$

```{r}
optimize(f=function(x){
  dbeta(x,2.7,6.3)},interval = c(0,1),maximum = T)$objective #M =2.7

Nsim <- 2500
alpha=2.7;beta=6.3
M <- 2.7
u <- runif(Nsim,max = M)
y <- runif(Nsim)
x <- y[u<dbeta(y,alpha,beta)]
w<-seq(0,1,.01)
hist(x,probability = TRUE)
lines(w,dbeta(w,alpha,beta),col=c("blue"))
```

## Método da Transformação (relações estocásticas)

$U \sim Gama (a, \beta)$ e $V \sim Gama (b, \beta)$ independetes. Então $U + V \sim Gama (a + b, \beta)$ e $U/(U+V) \sim Beta(a,b)$

```{r}
n <- 200
a <- 2
b <- 2
u <- rgamma(n, shape=a, rate=1)
v <- rgamma(n, shape=b, rate=1)
x <- u / (u + v)
q <- qbeta(ppoints(n), a, b)
qqplot(q, x, cex=0.5, xlab="Beta(2, 2)", ylab="amostra")
abline(0, 1,col="red")
```

## Mistura de Distribuições

Mistura de Normais $X: \frac{1}{3} X_1 + \frac{2}{3} X_2$ com $X_1 \sim N(0,1)$ e $X_2 \sim N(3,1)$

```{r}
n <- 1000
mu<-c(0,3)
k <- sample(1:2, size=n, replace=TRUE, prob=c(1/3,2/3))
m <- mu[k] # vector dim=n (mu_k1,...,mu_kn), elementos mu[1]=0 ou mu[2]=3
x <- rnorm(n, m, 1)
#par(mfrow = c(2, 2))

# plot da densidade da mistura dos NPAs
plot(density(x),xlim=c(-6,6),ylim=c(0,.5),
lwd=3,xlab="x",main="",col="grey20")
text(1.9,0.05,"1000 NPAs mistura",col="grey20")
```

## Distribuições multivariadas

### Normal multivariada

Utilizando Cholesky

```{r}	
rmvn.Choleski <- function(n, mu, Sigma) {
# generate n random vectors from MVN(mu, Sigma)
# dimension is inferred from mu and Sigma
d <- length(mu)
Q <- chol(Sigma) # Choleski factorization of Sigma
Z <- matrix(rnorm(n*d), nrow=n, ncol=d)
X <- Z %*% Q + matrix(mu, n, d, byrow=TRUE)
X
}
```

```{r}	
y <- subset(x=iris, Species=="virginica")[, 1:4]
mu <- colMeans(y)
mu
Sigma <- cov(y)
Sigma

#now generate MVN data with this mean and covariance
X <- rmvn.Choleski(200, mu, Sigma)
pairs(X)
```

# Otimização e Máxima Verossimilhança

## Método de Newton-Raphson

```{r}
g       <- function(x) log(x)/(1+x)
h       <-  function(x) ((x+1)*(1+1/x-log(x)))/(3+4/x+1/x^2-2*log(x)) 

my.newton = function(x0,g,h,tolerance=1e-3,max.iter=50) {
  x = x0
  iterations = 0
  made.changes = TRUE
  while(made.changes & (iterations < max.iter)) {
    x.old <- x
    iterations <- iterations +1
    made.changes <- FALSE
    x.new = x + h(x)
    relative.change = abs(x.new - x.old)/abs(x.old) 
    made.changes = (relative.change > tolerance)
    x = x.new
  }
  if (made.changes) {
    warning("Newton’s method terminated before convergence")
  }
  return(list(value=x,value=g(x),
              iterations=iterations,converged=!made.changes))
}


my.newton(x0=3,g=g,h=h,tolerance=1e-3,max.iter=50) 
```

# Algoritmo EM

## Passo a passo

input: $\theta_0$ (valor inicial), $y$ (dados observados), $z$ (dados não observados)

output: $\theta$ (estimativa de máxima verossimilhança)

1. Passo E (Expectation): Computar $E[Z|Y,\theta_0]$

2. Passo M (Maximization): Computar $\theta_1 = \arg\max_\theta L(\theta;Y,E[Z|Y,\theta_0])$

## Dados com censura do tipo I

```{r}	
a=170;n=300;theta=168 #-- verdadeiro theta --#
x <-rnorm(n,theta,1)
y <- x[x<a]
m <-length(y)
ybar <-mean(y)
sd <-sd(y)
ybar
sd
m

## Algoritmo EM
i=1
nonstop <- TRUE
theta[i] <- rnorm(1,mean=ybar,sd=sd(y))
cat("theta[",i,"]=",theta[i],"\n")

while (nonstop){
    theta[i+1] <- m*ybar/n+((n-m)/n)*(theta[i]+dnorm(a-theta[i])/(1-pnorm(a-theta[i])))
    i=i+1
    cat("theta[",i,"]=",theta[i],"\n")
    nonstop=(abs(theta[i]-theta[i-1])>10^(-5))
}
cat("# de iterações",i)
cat("theta^",theta[i],"\n")
```	

## Mistura de duas distribuições normais

```{r}
## Dados
y <- c(13.84823, 19.65983, 14.80328, 14.39590, 20.69053, 15.41869, 15.01747, 
      13.69639, 13.19747, 15.06224, 14.82386, 14.89709, 14.40141, 12.66398, 
      15.11618, 13.22798, 14.04956, 20.17014, 14.53800, 13.43051, 21.18457, 
      13.88862, 19.94610, 16.20979, 15.45938, 14.28910, 14.93497, 14.49772, 
      12.57415, 19.58430, 13.82636, 19.98711, 13.74862, 13.45034, 13.64232, 
      14.46726, 19.22776, 13.68149, 20.61726, 14.04241, 19.02825, 20.54842, 
      18.01239, 12.15557, 12.62941, 13.88048, 12.30432, 17.25600, 14.33063, 
      13.96460)
hist(y)

## Plotando os dados
plot(density(y))

## Obtendo valores iniciais
clasy <- kmeans(y,2)$cluster
mu10 <- mean(y[clasy==1])
mu20 <- mean(y[clasy==2])
sigma10 <- sd(y[clasy==1])
sigma20 <- sd(y[clasy==2])
pi10 <- sum(clasy==1)/length(clasy)
pi20 <- sum(clasy==2)/length(clasy)



## Implementando a função EM
EM.mixnormals <- function(y,mu,sigma,pi,tol=10e-7){

mu1    <- mu[1] 
mu2    <- mu[2]
sigma1 <- sigma[1]  
sigma2 <- sigma[2]
pi1    <- pi[1]
pi2    <- pi[2]

nonstop <- TRUE 
loglik  <- 0
i       <- 1

## valor inicial para a loglik
loglik[1] <- sum(log(pi1 * dnorm(y, mu1, sigma1) + pi2 * dnorm(y, mu2, sigma2)))

while(nonstop) {
  
  # E step
  num1 <- pi1 * dnorm(y, mu1, sigma1)
  num2 <- pi2 * dnorm(y, mu2, sigma2)
  den <- num1 + num2
  
  y1 <- num1/den
  y2 <- num2/den
  
  # M step
  pi1 <- sum(y1) / length(y)
  pi2 <- sum(y2) / length(y)
  
  mu1 <- sum(y1 * y) / sum(y1)
  mu2 <- sum(y2 * y) / sum(y2)
  
  sigma1 <- sqrt(sum(y1 * (y-mu1)^2) / sum(y1))
  sigma2 <- sqrt(sum(y2 * (y-mu2)^2) / sum(y2))
  
  i= i + 1
  
 loglik[i] <- sum(log(pi1 * dnorm(y, mu1, sigma1) + pi2 * dnorm(y, mu2, sigma2)))
  
  nonstop <- (abs(loglik[i]-loglik[i-1])>tol) 
}

return(list(mu = c(mu1,mu2), sigma=c(sigma1,sigma2), pi = c(pi1,pi2) ) ) 

}

## Rodando a função
resEM1 <- EM.mixnormals(y, mu = c(mu10,mu20), sigma= c(sigma10,sigma20), 
                        pi = c(pi10,pi20))
#resEM2 <- EM.mixnormals(x, mu = c(10,15), sigma= c(0.5,0.5), pi = c(0.7,0.3))

resEM1

#resEM2

num1_hat <- resEM1$pi[1] * dnorm(y, resEM1$mu[1], resEM1$sigma[1])
num2_hat <- resEM1$pi[2] * dnorm(y, resEM1$mu[2], resEM1$sigma[2])
den_hat <- num1_hat + num2_hat
y1_hat <- num1_hat/den_hat
y2_hat <- num2_hat/den_hat
head(cbind(y1_hat,y2_hat,(y1_hat+y2_hat)))

```

```{r}
#comparação com a função normalmixEM
pacman::p_load(mixtools)

resMix <- normalmixEM(y,k=2,lambda=c(pi10,pi20),mu=c(mu10,mu20),sigma=c(sigma10,sigma20))

resMix$lambda
resMix$mu
resMix$sigma


# comparação com a função mclust
pacman::p_load(mclust)

xdataMclust <- Mclust(y,G=2,modelNames = c("V"))
xdataMclust$parameters 

## Fazendo classificação com o mclust
plot(xdataMclust)
```

# Métodos de Monte Carlo

## Integração de Monte Carlo

```{r}
x <- seq(.1, 2.5, length = 10)
m <- 10000
u <- runif(m)
cdf <- numeric(length(x))
for (i in 1:length(x)) {
       g <- x[i] * exp(-(u * x[i])^2 / 2)
       cdf[i] <- mean(g) / sqrt(2 * pi) + 0.5
}

Phi <- pnorm(x)
print(round(rbind(x, cdf, Phi), 3))
```

::::