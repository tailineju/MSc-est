---
title: "Resumo - Prova 2"
subtitle: "Estatística Matemática"
author: "Tailine J. S. Nonato"
date: today
date-format: long
format: html
---

:::: {.panel-tabset}

# Content

| **Tipo de variáveis** | **Fórmula da convolução** |
|------------------------|----------------------------|
| Contínuas              | $f_Z(z) = \int_{-\infty}^{\infty} f_X(x)\, f_Y(z - x)\, dx$ |
| Discretas              | $p_Z(k) = \sum_{j} p_X(j)\, p_Y(k - j)$ |
| Mista                  | $f_Z(z) = \sum_k \mathbb{P}(X = k)\, f_Y(z - k)$ |



Para $Y = g(X)$ com $g$ injetora, $f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy}g^{-1}(y) \right|$.

Para $Z = g_1(X,Y)$ e $W = g_2(X,Y)$, $f_{Z,W}(z,w) = f_{X,Y}(h_1(z,w), h_2(z,w))\, |J(z,w)|$, onde $J$ é o Jacobiano da transformação inversa $(h_1, h_2)$.

# Lista 3

Disclaimer: Feito com NotebookLm, ferramenta de intelgência artificial do Google.

## Questão 1

Sejam X e Y variáveis aleatórias independentes com distribuição uniforme em $[\theta-0.5, \theta+0.5]$, $\theta \in R$. Verifique que a distribuição de X − Y não depende de $\theta$, encontrando sua densidade.

Conceitos-Chave: Variáveis aleatórias independentes, Distribuição Uniforme, Transformação de variáveis aleatórias (soma/diferença).

Método: Utilize o método da convolução para a diferença de duas variáveis aleatórias independentes. Se $X$ e $Y$ são independentes com densidades $f_X(x)$ e $f_Y(y)$, a densidade de $Z = X - Y$ é dada por $f_Z(z) = \int_{-\infty}^{\infty} f_X(x) f_Y(x-z) dx$.

**Passos da Solução:**

1. Escreva a densidade de $X$ e $Y$. Para $X \sim U[\theta-0.5, \theta+0.5]$, a densidade é $f_X(x) = 1$ para $x \in [\theta-0.5, \theta+0.5]$ e $0$ caso contrário. Similarmente para $Y$.

2. Aplique a fórmula da convolução. Observe que a duração do intervalo uniforme é 1 para ambas as variáveis.

3. A integração resultará em uma função de $z$ que não contém $\theta$, verificando assim que a distribuição não depende de $\theta$.

Resultado Esperado (Rpt): $f_{X-Y}(z) = \begin{cases} 1 + z, & -1 \le z < 0 \ 1 - z, & 0 \le z < 1 \ 0, & \text{caso contrário} \end{cases}$.

## Questão 2

Seja X uma variável aleatória cuja função de distribuição F é contínua e invertível na reta. Verifique que a distribuição de Y = F(X) é U.

Conceitos-Chave: Função de distribuição acumulada (FDA), Transformação de variáveis aleatórias, Distribuição Uniforme.

Método: Use o método da Função de Distribuição Acumulada para transformações de variáveis aleatórias. Para $Y=g(X)$, $F_Y(y) = P(Y \le y) = P(g(X) \le y)$.

Passos da Solução:

1. Para $Y = F(X)$, calcule $F_Y(y) = P(F(X) \le y)$.

2. Como $F$ é contínua e invertível (e por ser uma FDA, é não-decrescente), você pode aplicar a inversa $F^{-1}$ em ambos os lados da desigualdade: $P(F(X) \le y) = P(X \le F^{-1}(y))$.

3. Reconheça que $P(X \le F^{-1}(y))$ é, por definição, $F_X(F^{-1}(y))$. Dado que a variável aleatória é $X$ e sua FDA é $F$, então $F_X(x) = F(x)$.

4. Portanto, $F_Y(y) = F(F^{-1}(y)) = y$. Isso é a FDA da distribuição uniforme contínua em $$.

Insight: Este é um resultado fundamental em simulação, conhecido como o método da transformação inversa, que permite gerar variáveis aleatórias de qualquer distribuição contínua a partir de uma distribuição uniforme.

## Questão 3
Sejam $X ∼ Poisson(\alpha_1)$ e $Y ∼ Poisson(\alpha_2)$ variáveis aleatórias independentes. Verifique que X + Y ∼ Poisson($\alpha_1 + \alpha_2$).

Conceitos-Chave: Distribuição de Poisson, Variáveis aleatórias independentes, Soma de variáveis aleatórias, Função Característica.

Método: O método mais eficiente é usar as funções características. A função característica de uma soma de variáveis independentes é o produto de suas funções características.

Passos da Solução:

1. Encontre a função característica de uma variável aleatória Poisson. Para $X \sim \text{Poisson}(\lambda)$, $\varphi_X(t) = e^{\lambda(e^{it}-1)}$.

2. Use a propriedade de que para variáveis independentes $X$ e $Y$, $\varphi_{X+Y}(t) = \varphi_X(t)\varphi_Y(t)$.

3. Substitua e simplifique para mostrar que $\varphi_{X+Y}(t) = e^{(\alpha_1+\alpha_2)(e^{it}-1)}$.

4. Reconheça esta como a função característica de uma distribuição de Poisson com parâmetro $\alpha_1+\alpha_2$. A unicidade da função característica garante o resultado.

## Questão 4

Sejam X ∼ N($\mu_1, \sigma^2_1$) e Y ∼ N($\mu_2, \sigma^2_2$) variáveis aleatórias independentes. Verifique que X + Y ∼ N($\mu_1 + \mu_2, \sigma^2_1 + \sigma^2_2$).

Conceitos-Chave: Distribuição Normal, Variáveis aleatórias independentes, Soma de variáveis aleatórias, Função Característica.

Método: Similar ao Exercício 3, o método das funções características é o mais direto.

Passos da Solução:

1. Encontre a função característica de uma variável aleatória Normal. Para $X \sim N(\mu, \sigma^2)$, $\varphi_X(t) = e^{i\mu t - \sigma^2 t^2/2}$.

2. Multiplique as funções características de $X$ e $Y$ devido à independência.

3. O produto resultante será a função característica de uma distribuição Normal com média $\mu_1+\mu_2$ e variância $\sigma^2_1+\sigma^2_2$.

## Questão 5

Seja G o seguinte triângulo: Suponha que X e Y tenham densidade conjunta $f(x, y) = c\,1_G(x, y)$.

Importante: A descrição do triângulo na Lista 3 (Ex. 5) é ambígua e a figura subsequente na fonte é um diamante (Figura 39) para outro exercício (Ex. 29 na Lista 3). Assumindo que o triângulo para o Exercício 5 é a região $0 \le x, y$ e $x+y \le 1$ (com vértices (0,0), (1,0), (0,1)), proceda da seguinte forma:

**Conceitos-Chave:** Densidade conjunta, Densidade marginal, Independência de variáveis aleatórias, Transformação de variáveis (soma).

**Método:** Integração para obter constantes e densidades marginais. Para independência, verifique se $f_{X,Y}(x,y) = f_X(x)f_Y(y)$. Para a soma, use o método da FDA.

**Passos da Solução:**

**(a) Determine o valor da constante c.**

1. A densidade conjunta deve integrar a 1 sobre a região G. Calcule a área do triângulo (base 1, altura 1). Área $= 1/2$.
2. Portanto, $c \cdot (1/2) = 1 \implies c = 2$.

**(b) Calcule a distribuição de X, a de Y e a de Z = X + Y.**

1. Densidade de X: $f_X(x) = \int_{0}^{1-x} c\,dy = c(1-x)$ para $x \in [0,1]$. Com $c=2$, $f_X(x) = 2(1-x)$ para $0 \le x \le 1$.
2. Densidade de Y: Similarmente, $f_Y(y) = c(1-y)$ para $y \in [0,1]$. Com $c=2$, $f_Y(y) = 2(1-y)$ para $0 \le y \le 1$.
3. Distribuição de Z = X + Y:
    - Range de Z: Como $0 \le X,Y$ e $X+Y \le 1$, $Z$ varia de $0$ a $1$.
    - FDA de Z: $F_Z(z) = P(X+Y \le z) = \iint_{x+y \le z, x \ge 0, y \ge 0} c\,dx\,dy$. Para $0 \le z \le 1$, esta é a área de um triângulo menor com catetos $z$. Área $= z^2/2$.
    - $F_Z(z) = c \cdot z^2/2 = 2 \cdot z^2/2 = z^2$ para $0 \le z \le 1$.
    - $F_Z(z)=0$ para $z<0$ e $F_Z(z)=1$ para $z \ge 1$.

**(c) X e Y são independentes? Por quê?**

1. Verifique se $f_{X,Y}(x,y) = f_X(x)f_Y(y)$.
2. $2 \ne (2(1-x))(2(1-y)) = 4(1-x)(1-y)$.
3. Além disso, o suporte da densidade conjunta é triangular, enquanto o suporte do produto das marginais seria um quadrado unitário. Variáveis com densidade conjunta em uma região não retangular não podem ser independentes. Portanto, X e Y não são independentes.



## Questão 6

Se X e Y são as coordenadas de um ponto selecionado, ao acaso, do círculo unitário $\{(x, y) : x^2 + y^2 \le 1\}$, qual a distribuição da variável aleatória $Z = X^2 + Y^2$?

**Conceitos-Chave:** Distribuição uniforme em uma região, Transformação de variáveis aleatórias, Método da FDA.

**Método:** Use o método da Função de Distribuição Acumulada. $F_Z(z) = P(Z \le z) = P(X^2+Y^2 \le z)$.

**Passos da Solução:**

1. A densidade conjunta de $(X,Y)$ é $f_{X,Y}(x,y) = 1/\text{Área}$ sobre o círculo unitário. A área do círculo unitário é $\pi (1)^2 = \pi$. Então $f_{X,Y}(x,y) = 1/\pi$ para $x^2+y^2 \le 1$.
2. Para $z < 0$, $F_Z(z)=0$.
3. Para $0 \le z \le 1$: $P(X^2+Y^2 \le z)$ é a probabilidade de que o ponto $(X,Y)$ caia dentro de um círculo de raio $\sqrt{z}$. A área desse círculo menor é $\pi z$.
4. $F_Z(z) = (\pi z) / \pi = z$ para $0 \le z \le 1$.
5. Para $z > 1$, $F_Z(z)=1$.

**Resultado Esperado (Rpt):**
$$
F_Z(z) = \begin{cases}
0, & z < 0 \\
z, & 0 \le z < 1 \\
1, & z > 1
\end{cases}
$$
Esta é a FDA de uma distribuição uniforme contínua em $[0,1]$.



## Questão 7

Sejam $X \sim \text{Poisson}(5)$ e $Y \sim U$ variáveis aleatórias independentes. Ache a densidade de $Z = X + Y$.

**Conceitos-Chave:** Variáveis aleatórias discretas e contínuas, Distribuição de Poisson, Distribuição Uniforme, Convolução (soma de discretas e contínuas).

**Método:** A densidade da soma de uma variável aleatória discreta $X$ e uma contínua $Y$ (independentes) é dada por $f_Z(z) = \sum_{k} P(X=k) f_Y(z-k)$.

**Passos da Solução:**

1. A PMF de $X \sim \text{Poisson}(5)$ é $P(X=k) = e^{-5} 5^k / k!$ para $k=0, 1, 2, \dots$.
2. A densidade de $Y \sim U$ é $f_Y(y) = 1$ para $0 \le y \le 1$ e $0$ caso contrário.
3. $f_Y(z-k)=1$ apenas quando $0 \le z-k \le 1$, o que implica $k \le z \le k+1$. Isso significa que para um dado $z$, apenas um valor inteiro de $k$ (o piso de $z$, $k=\lfloor z \rfloor$) contribui para a soma.
4. Portanto, para $z > 0$, $f_Z(z) = P(X = \lfloor z \rfloor) \cdot 1 = e^{-5} 5^{\lfloor z \rfloor} / \lfloor z \rfloor!$.

**Resultado Esperado (Rpt):**
$$
f_Z(z) = \begin{cases}
0, & z < 0 \\
e^{-5} 5^{\lfloor z \rfloor} / \lfloor z \rfloor!, & z > 0
\end{cases}
$$



## Questão 8

Sejam $X \sim N(0, 1)$ e $Y \sim N(0, 1)$ variáveis aleatórias independentes. Verifique que $U = (X + Y)/\sqrt{2}$ e $V = (X - Y)/\sqrt{2}$ também são independentes e $N(0, 1)$.

**Conceitos-Chave:** Distribuição Normal Padrão, Variáveis aleatórias independentes, Transformação de vetores aleatórios, Método do Jacobiano.

**Método:** Use o método do Jacobiano para transformações de múltiplas variáveis.

**Passos da Solução:**

1. A densidade conjunta de $X$ e $Y$ é $f_{X,Y}(x,y) = \frac{1}{2\pi} e^{-(x^2+y^2)/2}$, já que são independentes $N(0,1)$.
2. Expresse $X$ e $Y$ em termos de $U$ e $V$: $X = (U+V)/\sqrt{2}$ e $Y = (U-V)/\sqrt{2}$.
3. Calcule o Jacobiano da transformação: $|\det| = 1$.
4. Substitua $X$ e $Y$ em $f_{X,Y}(x,y)$ e multiplique por $|J|$. A soma dos quadrados $x^2+y^2$ se transforma em $u^2+v^2$.
5. A densidade conjunta de $U$ e $V$ será $f_{U,V}(u,v) = \frac{1}{2\pi} e^{-(u^2+v^2)/2}$.
6. Como a densidade conjunta se fatoriza em um produto de densidades marginais $N(0,1)$, $U$ e $V$ são independentes e ambos seguem uma distribuição $N(0,1)$.



## Questão 9

Sejam $X \sim U$ e $Y \sim U$ variáveis aleatórias independentes.

**Conceitos-Chave:** Variáveis aleatórias independentes, Distribuição Uniforme, Transformação de vetores aleatórios, Método do Jacobiano, Independência de variáveis aleatórias.

**Método:** Use o método do Jacobiano. Para a independência, verifique a fatorização da densidade conjunta e/ou a forma do suporte.

**Passos da Solução:**

**(a) Ache a densidade conjunta de $W = X + Y$ e $Z = X - Y$.**

1. A densidade conjunta de $X$ e $Y$ é $f_{X,Y}(x,y)=1$ para $0 \le x,y \le 1$.
2. As transformações inversas são $X=(W+Z)/2$ e $Y=(W-Z)/2$.
3. O Jacobiano da transformação é $|J|=1/2$.
4. Defina a região de suporte para $(W,Z)$ transformando o quadrado unitário $[0,1] \times [0,1]$:
    - $0 \le (W+Z)/2 \le 1 \implies 0 \le W+Z \le 2$.
    - $0 \le (W-Z)/2 \le 1 \implies 0 \le W-Z \le 2$.
    - Isso define um losango (diamante) com vértices (0,0), (1,1), (2,0), (1,-1) no plano $(W,Z)$.
5. A densidade conjunta de $W$ e $Z$ é $f_{W,Z}(w,z) = 1/2$ sobre a região G definida.

**(b) As variáveis $W$ e $Z$ são independentes?**

1. Como a região de suporte G para $(W,Z)$ não é retangular (é um losango), as variáveis $W$ e $Z$ não são independentes. Para variáveis independentes, o suporte conjunto deve ser um retângulo e a densidade conjunta deve ser o produto das marginais.



## Questão 10

Suponha que $X \sim U$. Calcule a densidade de $Y = X^4$ e a de $Z = 1/X$. As variáveis $Y$ e $Z$ possuem densidade conjunta? Por quê?

**Conceitos-Chave:** Transformação de variáveis aleatórias, Densidade de probabilidade, Distribuições singulares.

**Método:** Use o método da FDA para as densidades marginais. Para a densidade conjunta de funções da mesma variável, considere a dependência funcional.

**Passos da Solução:**

**Densidade de $Y = X^4$:**

1. Para $y \in [0,1]$, $F_Y(y) = P(X^4 \le y) = P(X \le y^{1/4}) = y^{1/4}$ (já que $X \sim U$).
2. $f_Y(y) = dF_Y(y)/dy = (1/4)y^{-3/4}$ para $y \in (0,1)$ e $0$ caso contrário.

**Densidade de $Z = 1/X$:**

1. Para $z \ge 1$ (pois $X \in [0,1]$), $F_Z(z) = P(1/X \le z) = P(X \ge 1/z) = 1 - P(X < 1/z) = 1 - (1/z)$.
2. $f_Z(z) = dF_Z(z)/dz = 1/z^2$ para $z \ge 1$ e $0$ caso contrário.

**Densidade conjunta de $Y$ e $Z$:**

1. As variáveis $Y$ e $Z$ são funcionalmente dependentes: $Z = 1/X = 1/(Y^{1/4}) = Y^{-1/4}$.
2. Como $Y$ e $Z$ são funções da mesma variável aleatória $X$, a distribuição conjunta de $(Y,Z)$ reside em uma curva no plano $(y,z)$, ou seja, $z = y^{-1/4}$.
3. Isso significa que a distribuição conjunta é singular e não possui uma densidade conjunta no sentido usual sobre $\mathbb{R}^2$. A probabilidade de $(Y,Z)$ cair em qualquer região de área não-nula que não esteja na curva é 0.

## Questão 11

Seja X uma variável aleatória possuindo densidade $f(x)$. Ache a densidade de $Y = |X|$.

**Conceitos-Chave:** Transformação de variáveis aleatórias, Densidade de probabilidade, Método da FDA.

**Método:** Use o método da Função de Distribuição Acumulada.

**Passos da Solução:**
1. Para $y < 0$, $F_Y(y) = P(|X| \le y) = 0$.
2. Para $y \ge 0$, $F_Y(y) = P(|X| \le y) = P(-y \le X \le y) = F_X(y) - F_X(-y)$ (assumindo $F_X$ contínua nos pontos relevantes).
3. A densidade $f_Y(y) = dF_Y(y)/dy$. Usando a regra da cadeia, $f_Y(y) = f_X(y) - f_X(-y)(-1) = f_X(y) + f_X(-y)$ para $y > 0$.

**Resultado Esperado (Rpt):**
$$
f_Y(y) = \begin{cases}
f_X(y) + f_X(-y), & y > 0 \\
0, & \text{caso contrário}
\end{cases}
$$



## Questão 12

Sejam $X \sim \exp(\lambda)$ e $Y \sim \exp(\lambda)$ variáveis aleatórias independentes. Verifique que $Z = X/(X + Y) \sim U$.

**Conceitos-Chave:** Distribuição Exponencial, Variáveis aleatórias independentes, Transformação de vetores aleatórios, Método do Jacobiano.

**Método:** Use o método do Jacobiano. Defina uma segunda variável auxiliar ($W = X+Y$) para transformar de $(X,Y)$ para $(Z,W)$.

**Passos da Solução:**
1. A densidade conjunta de $X$ e $Y$ é $f_{X,Y}(x,y) = \lambda^2 e^{-\lambda(x+y)}$ para $x,y > 0$.
2. Defina $Z = X/(X+Y)$ e $W = X+Y$.
3. As transformações inversas são $X = ZW$ e $Y = W(1-Z)$.
4. Calcule o Jacobiano da transformação: $|J| = w$.
5. O domínio é $0 < Z < 1$, $W > 0$.
6. A densidade conjunta de $Z$ e $W$ é $f_{Z,W}(z,w) = \lambda^2 w e^{-\lambda w}$.
7. Para obter a densidade marginal de $Z$, integre $f_{Z,W}(z,w)$ em relação a $w$: $f_Z(z) = \int_0^\infty \lambda^2 w e^{-\lambda w} dw = 1$.
8. Portanto, $f_Z(z) = 1$ para $0 < z < 1$ e $0$ caso contrário, que é a densidade de uma distribuição $U$.



## Questão 13

Se $X$ possui densidade $f_X(x) = f(x)$, $-\pi/2 < x < \pi/2$. Qual a densidade de $Y = \cos(X)$?

**Conceitos-Chave:** Transformação de variáveis aleatórias, Função não-injetora.

**Método:** Use o método da FDA para $Y=g(X)$, considerando os dois ramos de $\cos(x)$.

**Passos da Solução:**
1. Para $y \in (0,1]$, a equação $y=\cos(x)$ tem duas soluções: $x_1 = -\arccos(y)$ e $x_2 = \arccos(y)$.
2. O módulo da derivada inversa é $|dx/dy| = 1/\sqrt{1-y^2}$.
3. A densidade de $Y$ é a soma das contribuições de cada ramo:
$$
f_Y(y) = \frac{1}{\sqrt{1-y^2}} [f(-\arccos(y)) + f(\arccos(y))]
$$
para $0 < y < 1$.

**Resultado Esperado (Rpt):**
$$
f_Y(y) = \begin{cases}
\frac{1}{\sqrt{1-y^2}} [f(-\arccos(y)) + f(\arccos(y))], & 0 < y < 1 \\
0, & \text{caso contrário}
\end{cases}
$$



## Questão 14

Se $X$ é uma variável aleatória com densidade $f_X(x) = \frac{2x}{\pi^2}$, $0 < x < \pi$. Qual a densidade de $Y = \sin(X)$?

**Conceitos-Chave:** Transformação de variáveis aleatórias, Função não-injetora.

**Método:** Use o método da FDA, considerando os múltiplos ramos da inversa.

**Passos da Solução:**
1. Para $y \in (0,1]$, as soluções são $x_1 = \arcsin(y)$ e $x_2 = \pi - \arcsin(y)$.
2. O módulo da derivada inversa é $1/\sqrt{1-y^2}$.
3. A densidade de $Y$ é:
$$
f_Y(y) = \frac{2}{\pi^2\sqrt{1-y^2}} [\arcsin(y) + \pi - \arcsin(y)] = \frac{2}{\pi\sqrt{1-y^2}}
$$
para $0 < y < 1$.

**Resultado Esperado (Rpt):**
$$
f_Y(y) = \begin{cases}
\frac{2}{\pi\sqrt{1-y^2}}, & 0 < y < 1 \\
0, & \text{caso contrário}
\end{cases}
$$



## Questão 15

Sejam $X \sim U$ e $Y \sim U$ variáveis aleatórias independentes, e sejam $R = \sqrt{-2 \log(1-X)}$ e $\Theta = \pi(2Y - 1)$.

**Conceitos-Chave:** Transformação de variáveis aleatórias, Distribuição Uniforme, Distribuição de Rayleigh, Distribuição Normal, Método do Jacobiano (Box-Muller).

**Passos da Solução:**

(a) Verifique que $\Theta \sim U[-\pi, \pi]$ e que $R$ tem distribuição de Rayleigh com densidade $f(r) = r e^{-r^2/2}$, $r > 0$.

(b) Verifique que $Z = R \cos(\Theta)$ e $W = R \sin(\Theta)$ também são independentes e $N(0, 1)$.



## Questão 16

Sejam $X \sim \text{Gama}(\alpha_1, 1)$ e $Y \sim \text{Gama}(\alpha_2, 1)$ variáveis aleatórias independentes, com $\alpha_1 > 0$ e $\alpha_2 > 0$. Verifique que $X + Y$ e $X/Y$ são independentes e ache as suas distribuições.

**Conceitos-Chave:** Distribuição Gama, Variáveis aleatórias independentes, Transformação de vetores aleatórios, Método do Jacobiano, Distribuição Beta-prima.

**Passos da Solução:**
- $U = X + Y \sim \text{Gama}(\alpha_1+\alpha_2, 1)$
- $V = X/Y$ tem distribuição Beta-prima: $f_V(v) = \frac{\Gamma(\alpha_1+\alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)} v^{\alpha_1-1} (1+v)^{-(\alpha_1+\alpha_2)}$
- $U$ e $V$ são independentes.



## Questão 17

Seja $(X,Y)$ o vetor aleatório com distribuição uniforme sobre o triângulo $\{(x, y) \in \mathbb{R}^2 : 0 \le x \le y \le 1\}$; isto é, $(X,Y)$ tem função de densidade conjunta $f(x, y) = 2$, $0 \le x \le y \le 1$. Determine a densidade de $Z = X + Y$.

**Resultado Esperado (Rpt):**
$$
f_Z(z) = \begin{cases}
z, & 0 \le z \le 1 \\
2 - z, & 1 \le z \le 2 \\
0, & \text{caso contrário}
\end{cases}
$$



## Questão 18

Suponha que o vetor aleatório $(X,Y)$ tem função de densidade conjunta $f(x, y) = e^{-x}$, $0 \le y \le x < \infty$.

(a) Determine a densidade conjunta de $W = X + Y$ e $Z = X - Y$.

- Transformações inversas: $X = (W+Z)/2$, $Y = (W-Z)/2$
- Jacobiano: $|J| = 1/2$
- Suporte: $0 \le Z \le W < \infty$
- $f_{W,Z}(w,z) = \frac{1}{2} e^{-(w+z)/2}$

(b) Densidades marginais:
- $f_W(w) = e^{-w/2}(1-e^{-w/2})$ para $w > 0$
- $f_Z(z) = e^{-z}$ para $z > 0$

## Questão 19

A variável aleatória $X$ tem distribuição de Weibull se possui densidade $f(x) = \lambda \alpha x^{\alpha-1} e^{-\lambda x^\alpha}$, $x > 0$. Determine $E(X)$.

**Conceitos-Chave:** Expectância (Esperança Matemática), Densidade de probabilidade.

**Método:** Calcule a esperança pela integral $E[X] = \int_0^\infty x f_X(x) dx$.

**Passos da Solução:**

1. Substitua a densidade: $E[X] = \int_0^\infty x (\lambda \alpha x^{\alpha-1} e^{-\lambda x^\alpha}) dx = \int_0^\infty \lambda \alpha x^\alpha e^{-\lambda x^\alpha} dx$.
2. Use a substituição $u = \lambda x^\alpha$. Então $x = (u/\lambda)^{1/\alpha}$ e $du = \lambda \alpha x^{\alpha-1} dx$.
3. A integral se transforma em $E[X] = \int_0^\infty (u/\lambda)^{1/\alpha} e^{-u} du = \lambda^{-1/\alpha} \int_0^\infty u^{1/\alpha} e^{-u} du$.
4. A integral $\int_0^\infty u^{s-1} e^{-u} du$ é a função Gama $\Gamma(s)$. Então $\int_0^\infty u^{1/\alpha} e^{-u} du = \Gamma(1/\alpha+1)$.

**Resultado Esperado (Rpt):** $E(X) = \Gamma(1+1/\alpha)/\lambda^{1/\alpha}$.



## Questão 20

Sejam $X \sim U$ e $Y \sim U$ variáveis aleatórias independentes. Calcule $E(Z)$ e $E(W)$, onde $Z = \min\{X,Y\}$ e $W = \max\{X,Y\}$.

**Conceitos-Chave:** Ordem estatística, Distribuição Uniforme, Expectância.

**Método:**

- Para $Z = \min\{X,Y\}$:
    1. A FDA de $Z$ é $F_Z(z) = P(\min\{X,Y\} \le z) = 1 - P(X>z, Y>z)$.
    2. Devido à independência, $P(X>z, Y>z) = (1-z)^2$ para $z \in [0,1]$.
    3. $F_Z(z) = 1 - (1-z)^2 = 2z-z^2$.
    4. A densidade $f_Z(z) = dF_Z(z)/dz = 2-2z$ para $z \in [0,1]$.
    5. $E[Z] = \int_0^1 z(2-2z) dz = 1/3$.

- Para $W = \max\{X,Y\}$:
    1. A FDA de $W$ é $F_W(w) = P(\max\{X,Y\} \le w) = w^2$ para $w \in [0,1]$.
    2. A densidade $f_W(w) = dF_W(w)/dw = 2w$ para $w \in [0,1]$.
    3. $E[W] = \int_0^1 w(2w) dw = 2/3$.



## Questão 21

Um jogador vai lançando uma moeda honesta. Ele para depois de lançar ou duas caras sucessivas ou duas coroas sucessivas. Seja $X$ o número de lançamentos.

**Conceitos-Chave:** Variável aleatória discreta, Espaço amostral, Expectância.

**Método:** Defina a distribuição de probabilidade de $X$ ($p_X(k)$).

**Passos da Solução:**

(a) Encontre a distribuição de $X$.

1. $P(X=2)$: A sequência pode ser HH ou TT. $P(X=2) = (1/2)^2 + (1/2)^2 = 1/2$.
2. $P(X=3)$: A sequência deve ser THH ou HTT. $P(X=3) = (1/2)^3 + (1/2)^3 = 1/4$.
3. $P(X=k)$ para $k > 3$: A sequência deve alternar até $k-1$ e então o $k$-ésimo lançamento deve ser igual ao $(k-1)$-ésimo. $p_X(k) = 2 \cdot (1/2)^k = 1/2^{k-1}$ para $k \ge 2$.

(b) Determine $E(X)$.

1. $E[X] = \sum_{k=2}^\infty k \cdot p_X(k) = \sum_{k=2}^\infty k \cdot (1/2)^{k-1}$.
2. Usando séries geométricas, $E[X] = 3$.



## Questão 22

Uma urna contém 3 bolas numeradas 1, 2, 3. Uma pessoa tira uma bola e a devolve, tira outra e a devolve, continuando até tirar uma bola pela segunda vez. Seja $X$ o número total de retiradas necessárias para obter essa repetição.

**Conceitos-Chave:** Variável aleatória discreta, Probabilidade, Expectância.

**Método:** Defina a distribuição de probabilidade de $X$ ($p_X(k)$).

**Passos da Solução:**

(a) Encontre a distribuição de $X$.

- $P(X=2) = 1/3$
- $P(X=3) = 4/9$
- $P(X=4) = 2/9$

(b) Determine $E(X)$.

- $E[X] = 2 \cdot (1/3) + 3 \cdot (4/9) + 4 \cdot (2/9) = 26/9$



## Questão 23

Suponha que a variável aleatória $X$ tenha a seguinte densidade triangular:
$$
f(x) = \begin{cases}
1 + x, & -1 \le x \le 0 \\
1 - x, & 0 < x \le 1 \\
0, & \text{caso contrário}
\end{cases}
$$
Calcule $E(X)$ e $Var(X)$.

**Conceitos-Chave:** Expectância, Variância, Densidade de probabilidade.

**Método:** Use as definições de esperança e variância para variáveis aleatórias contínuas.

**Passos da Solução:**

- $E[X] = 0$ (simetria)
- $Var(X) = 1/6$



## Questão 24

Encontre as variâncias de $Z$ e $W$ no Exercício 20.

**Conceitos-Chave:** Variância, Ordem estatística, Distribuição Uniforme.

**Método:** Use $Var(X) = E[X^2] - (E[X])^2$.

**Passos da Solução:**

- $Var(Z) = 1/18$
- $Var(W) = 1/18$



## Questão 25

Através de experimentos estatísticos, determina-se que a duração de um certo tipo de chamada telefônica satisfaz a relação $P(T > t) = ae^{-\lambda t} + (1 − a)e^{-\xi t}$, $t > 0$, com $0 \le a \le 1$, $\lambda > 0$, $\xi > 0$. Encontre a média e a variância de $T$.

**Conceitos-Chave:** Função de Sobrevivência, Expectância, Variância, Distribuição exponencial.

**Método:** Para variáveis aleatórias não negativas, $E[T] = \int_0^\infty P(T>t) dt$.

**Passos da Solução:**

- $E[T] = a/\lambda + (1-a)/\xi$
- $Var(T) = 2a/\lambda^2 + 2(1-a)/\xi^2 - (a/\lambda + (1-a)/\xi)^2$



## Questão 26

Calcule a esperança e a variância da variável aleatória $X$, com:

**Conceitos-Chave:** Expectância, Variância, Propriedades das distribuições comuns.

**Passos da Solução (Resultados Padrão):**

- (a) $X \sim \text{Poisson}(\lambda)$: $E[X]=\lambda$, $Var(X)=\lambda$
- (b) $X \sim \text{binomial}(n, p)$: $E[X]=np$, $Var(X)=np(1-p)$
- (c) $X \sim N(\mu, \sigma^2)$: $E[X]=\mu$, $Var(X)=\sigma^2$
- (d) $X \sim \text{Gama}(\alpha, \beta)$: $E[X]=\alpha/\beta$, $Var(X)=\alpha/\beta^2$
- (e) $X \sim U[a, b]$: $E[X]=(a+b)/2$, $Var(X)=(b-a)^2/12$



## Questão 27

Sejam $X \in \{0, 1\}$ e $Y \in \{0, 1\}$ duas variáveis aleatórias dicotômicas. Se $E(XY) = E(X)E(Y)$, então $X$ e $Y$ são independentes?

**Conceitos-Chave:** Independência de variáveis aleatórias, Expectância, Covariância.

**Método:** Verifique a definição de independência para todas as combinações de valores.

**Passos da Solução:**

- Para variáveis dicotômicas, $E[X] = P(X=1)$, $E[Y]=P(Y=1)$, e $E[XY]=P(X=1, Y=1)$.
- A condição $E[XY] = E[X]E[Y]$ implica independência para todas as combinações.

**Resultado Esperado (Rpt):** Sim, são independentes.


## Questão 28

Seja $X \sim \text{Bernoulli}(p)$, $Y = 1-X$ e $Z = XY$.

**Conceitos-Chave:** Distribuição de Bernoulli, Variável aleatória discreta, Função de probabilidade conjunta, Independência de variáveis aleatórias.

**Método:** Crie tabelas de probabilidade conjunta para as variáveis.

**Passos da Solução:**

(a) Encontre as seguintes funções de probabilidade conjunta: $P(X = x, Y = y)$ e $P(X = x, Z = z)$, para cada $x, y, z$.

1. Para $X \sim \text{Bernoulli}(p)$, $P(X=1)=p$ e $P(X=0)=1-p$.

2. $P(X=x, Y=y)$:
    - Se $X=1$, então $Y=0$. $P(X=1, Y=0) = P(X=1) = p$.
    - Se $X=0$, então $Y=1$. $P(X=0, Y=1) = P(X=0) = 1-p$.
    - Todas as outras combinações têm probabilidade 0.

    | X | Y | $P(X, Y)$ |
    |---|---|---------|
    | 0 | 0 |     0     |
    | 0 | 1 |   $1-p$   |
    | 1 | 0 |     $p$   |
    | 1 | 1 |     0     |

3. $P(X=x, Z=z)$:
    - $Y=1-X$, então $Z=XY = X(1-X)$. Se $X=1$, $Z=0$; se $X=0$, $Z=0$. Portanto, $Z$ é sempre 0.
    - $P(X=0, Z=0) = 1-p$.
    - $P(X=1, Z=0) = p$.
    - Todas as outras combinações têm probabilidade 0.

    | X | Z | $P(X, Z)$ |
    |---|---|---------|
    | 0 | 0 |   $1-p$   |
    | 1 | 0 |     $p$   |

(b) $X$ e $Y$ são independentes?

- Verifique se $P(X=x, Y=y) = P(X=x)P(Y=y)$.
- Por exemplo, $P(X=0, Y=0) = 0$, mas $P(X=0)P(Y=0) = (1-p)p$.
- Como $0 \ne (1-p)p$ (para $p \in (0,1)$), $X$ e $Y$ não são independentes.

(c) $X$ e $Z$ são independentes?

- $P(Z=0) = 1$ (pois $Z$ só pode ser 0).
- $P(X=0, Z=0) = 1-p$ e $P(X=0)P(Z=0) = (1-p)\cdot 1 = 1-p$.
- $P(X=1, Z=0) = p$ e $P(X=1)P(Z=0) = p\cdot 1 = p$.
- Portanto, $X$ e $Z$ são independentes.

## Questão 29

Seja $(X,Y)$ uniformemente distribuído na região $G$ dada na seguinte figura: (Assume Figura 39 - diamante)

**Conceitos-Chave:** Densidade uniforme conjunta, Densidades marginais, Covariância.

**Ambiguidade:** A "seguinte figura" referida na Lista 3 (Ex. 29) é a Figura 39 no livro-texto, que é uma região em forma de diamante com vértices $(-1,0)$, $(0,1)$, $(1,0)$, $(0,-1)$. A área dessa região é 2. Portanto, a densidade uniforme $f_{X,Y}(x,y) = 1/2$ sobre $G$. Contraditoriamente, os resultados (Rpt) para as densidades marginais $f_X(x)$ e $f_Y(y)$ implicam uma região de suporte diferente, não uniforme ou não correspondente ao diamante. Este guia seguirá o que a figura 39 indica para o cálculo da constante $c$ e da Covariância, e notará a inconsistência com as marginais fornecidas na Rpt.

**Passos da Solução:**

- **Área da Região $G$ (Figura 39):** A região é um diamante, com diagonais de comprimento 2. A área é $1/2 \cdot d_1 \cdot d_2 = 1/2 \cdot 2 \cdot 2 = 2$.
- **Constante $c$:** Se a distribuição é uniforme, $c = 1/\text{Área} = 1/2$.

**(a) Encontre as densidades marginais de $X$ e $Y$.**

1. Densidade de $X$: $f_X(x) = \int_{-(1-|x|)}^{1-|x|} (1/2) dy = (1/2) \cdot [y]_{-(1-|x|)}^{1-|x|} = (1/2) \cdot (1-|x| - (-(1-|x|))) = (1/2) \cdot 2(1-|x|) = 1-|x|$ para $x \in [-1,1]$ e $0$ caso contrário.
2. Densidade de $Y$: Devido à simetria do diamante, $f_Y(y)$ é idêntica a $f_X(x)$: $f_Y(y) = 1-|y|$ para $y \in [-1,1]$ e $0$ caso contrário.
3. **Inconsistência:** O Rpt para $f_X(x)$ e $f_Y(y)$ são diferentes destas, sugerindo que a figura ou o tipo de densidade uniforme não são os que o Rpt usa. No entanto, para o cálculo da Covariância, o valor 0 é consistente com a simetria da região diamante.

**(b) Ache $\mathrm{Cov}(X,Y)$.**

1. $\mathrm{Cov}(X,Y) = E[XY] - E[X]E[Y]$.
2. Devido à simetria da região $G$ em torno de ambos os eixos e o fato de $f_{X,Y}(x,y)$ ser uniforme (constante), $E[X]=0$ e $E[Y]=0$.
3. $E[XY] = \iint_G xy \cdot (1/2) dx dy$. Devido à simetria da região $G$ e o comportamento da função $g(x,y)=xy$ (que é positiva nos quadrantes I e III e negativa nos quadrantes II e IV), a integral de $xy$ sobre o diamante será 0.
4. Portanto, $E[XY]=0$.
5. $\mathrm{Cov}(X,Y) = 0 - 0 \cdot 0 = 0$.



## Questão 30
Sejam $X \sim U$ e $Y \sim U$ variáveis aleatórias independentes, e sejam $X_{(1)} = \min\{X,Y\}$, $X_{(2)} = \max\{X,Y\}$. Determine o coeficiente de correlação $\rho_{X_{(1)},X_{(2)}}$.

**Conceitos-Chave:** Ordem estatística, Distribuição Uniforme, Coeficiente de Correlação.

**Método:** Use a definição do coeficiente de correlação: $\rho_{A,B} = \mathrm{Cov}(A,B) / (\sigma_A \sigma_B)$.

**Passos da Solução:**

1. No Exercício 20, calculamos:
    - $E[X_{(1)}] = 1/3$ e $E[X_{(2)}] = 2/3$.
    - $\mathrm{Var}(X_{(1)}) = 1/18$ e $\mathrm{Var}(X_{(2)}) = 1/18$.
2. Calculamos $\mathrm{Cov}(X_{(1)}, X_{(2)})$. Note que $X_{(1)}X_{(2)} = \min\{X,Y\}\max\{X,Y\} = XY$.
3. Como $X$ e $Y$ são independentes e $U$, $E[XY] = E[X]E[Y] = (1/2)(1/2) = 1/4$.
4. $\mathrm{Cov}(X_{(1)}, X_{(2)}) = E[X_{(1)}X_{(2)}] - E[X_{(1)}]E[X_{(2)}] = 1/4 - (1/3)(2/3) = 1/4 - 2/9 = (9-8)/36 = 1/36$.
5. $\rho_{X_{(1)}, X_{(2)}} = \frac{1/36}{\sqrt{1/18 \cdot 1/18}} = \frac{1/36}{1/18} = 1/2$.



## Questão 31

Sejam $X,Y$ e $Z$ variáveis aleatórias independentes com distribuição comum $U$. Determine a esperança e a variância de $W = (X + Y)Z$.

**Conceitos-Chave:** Variáveis aleatórias independentes, Expectância de funções de variáveis aleatórias, Variância de funções de variáveis aleatórias.

**Método:** Use as propriedades de linearidade da esperança e da independência para simplificar os cálculos.

**Passos da Solução:**

- $E(W)$:
    1. $W = (X+Y)Z$. Como $Z$ é independente de $(X+Y)$, $E[W] = E[(X+Y)Z] = E[X+Y]E[Z]$.
    2. Como $X$ e $Y$ são independentes, $E[X+Y] = E[X]+E[Y]$.
    3. Para $X,Y,Z \sim U$, $E[X]=E[Y]=E[Z]=1/2$.
    4. $E[W] = (1/2+1/2) \cdot (1/2) = 1 \cdot (1/2) = 1/2$.

- $\mathrm{Var}(W)$:
    1. $\mathrm{Var}(W) = E[W^2] - (E[W])^2 = E[(X+Y)^2Z^2] - (1/2)^2$.
    2. Como $Z$ é independente de $(X+Y)$, $E[(X+Y)^2Z^2] = E[(X+Y)^2]E[Z^2]$.
    3. Para $Z \sim U$, $E[Z^2] = \int_0^1 z^2 \cdot 1 dz = 1/3$.
    4. $E[(X+Y)^2] = E[X^2+2XY+Y^2]$. Como $X,Y$ são independentes, $E[XY]=E[X]E[Y]$.
    5. $E[X^2] = \int_0^1 x^2 \cdot 1 dx = 1/3$, e $E[Y^2]=1/3$.
    6. $E[(X+Y)^2] = E[X^2]+2E[X]E[Y]+E[Y^2] = 1/3 + 2(1/2)(1/2) + 1/3 = 1/3 + 1/2 + 1/3 = 7/6$.
    7. $\mathrm{Var}(W) = (7/6)(1/3) - (1/2)^2 = 7/18 - 1/4 = (14-9)/36 = 5/36$.



## Questão 32

Seja $\rho$ o coeficiente de correlação entre as variáveis aleatórias $X$ e $Y$. Se $Z = aX + b$ e $W = cY + d$, com $a \ne 0$ e $c \ne 0$, calcule $\rho_{Z,W}$.

**Conceitos-Chave:** Coeficiente de Correlação, Propriedades de Covariância e Variância sob transformações lineares.

**Método:** Use as propriedades de $\mathrm{Cov}(aX+b, cY+d) = ac\,\mathrm{Cov}(X,Y)$ e $\sigma_{aX+b} = |a|\sigma_X$.

**Passos da Solução:**

1. $\mathrm{Cov}(Z,W) = \mathrm{Cov}(aX+b, cY+d) = ac\,\mathrm{Cov}(X,Y)$.
2. $\sigma_Z = \sqrt{\mathrm{Var}(aX+b)} = \sqrt{a^2\mathrm{Var}(X)} = |a|\sigma_X$.
3. $\sigma_W = \sqrt{\mathrm{Var}(cY+d)} = \sqrt{c^2\mathrm{Var}(Y)} = |c|\sigma_Y$.
4. $\rho_{Z,W} = \frac{\mathrm{Cov}(Z,W)}{\sigma_Z \sigma_W} = \frac{ac\,\mathrm{Cov}(X,Y)}{|a|\sigma_X |c|\sigma_Y} = \frac{ac}{|a||c|} \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y} = \text{sgn}(ac) \rho_{X,Y}$.

**Resultado Esperado (Rpt):** $\rho_{Z,W} = \rho$. Este resultado implica que $a$ e $c$ têm o mesmo sinal (ou seja, $ac > 0$), de modo que $\text{sgn}(ac)=1$. Se tivessem sinais opostos, o coeficiente de correlação seria $-\rho$.


## Questão 33

 Sejam $U$, $T$ e $X$ três variáveis aleatórias (absolutamente) contínuas e não negativas. Denote por $G_U$ e $f_T$ as correspondentes funções de distribuição acumulada e de densidade para $U$ e $T$. Para $X$ defina a função real $F_X$ como segue:

**Conceitos-Chave:** Função de Distribuição Acumulada (FDA), Expectância, Desigualdade de Markov.

**Método:**

- (a) Verifique que $F_X$ é a função de distribuição acumulada de $X$.
    1. Verifique as propriedades de uma FDA: não-decrescente, limite para $-\infty$ é 0, limite para $+\infty$ é 1, e continuidade à direita.
    2. $F_X(x) = \int_0^{G_U(x)/(1-G_U(x))} f_T(t) dt$ para $x > 0$ e $0$ para $x \le 0$.
    3. Note que $G_U(x)$ é a FDA de $U$.

- (b) Se além do mais, $E(T) < \infty$, $E(U^p) < \infty$, $\forall p > 0$, e existe $a > 0$ tal que $G_U(a) > 0$, verifique que $E(X^p) <\infty$.
    1. Dica do Rpt: Use a Desigualdade de Markov. A Desigualdade de Markov afirma que para uma variável aleatória não-negativa $X$ e $a > 0$, $P(X \ge a) \le E[X]/a$.
    2. A ideia é usar as informações dadas sobre $E[T]$ e $E[U^p]$ para mostrar que $E[X^p]$ é finito. Este é um problema mais avançado, que provavelmente requer a aplicação de teoremas de convergência ou técnicas de integração mais complexas, dada a natureza da definição de $F_X(x)$.

# Lista 4 

Disclaimer: Feito com NotebookLm, ferramenta de intelgência artificial do Google.

## Questão 1

1. Sejam $X_1$ e $X_2$ variáveis aleatórias independentes com distribuição geométrica de parâmetro $0 < p < 1$: $P(X_i = k) = p(1−p)^k$, $k = 0, 1, 2, \dots$; $i = 1, 2$.

(a) Calcule $P(X_1 = X_2)$ e $P(X_1 < X_2)$.

(b) Determine a distribuição condicional de $X_1$ dada $X_1 + X_2$.

**Resposta:**

(a) $P(X_1 = X_2) = \dfrac{p}{2-p}$ e $P(X_1 < X_2) = \dfrac{1-p}{2-p}$.

Para calcular $P(X_1 = X_2)$, usamos a lei da probabilidade total, somando sobre todos os valores possíveis de $k$:
$$
P(X_1 = X_2) = \sum_{k=0}^{\infty} P(X_1 = k, X_2 = k) = \sum_{k=0}^{\infty} [p(1-p)^k]^2 = p^2 \sum_{k=0}^{\infty} (1-p)^{2k}
$$
Esta é uma série geométrica de razão $r = (1-p)^2$:
$$
\sum_{k=0}^{\infty} r^k = \frac{1}{1-r}
$$
Logo,
$$
P(X_1 = X_2) = p^2 \cdot \frac{1}{1-(1-p)^2} = p^2 \cdot \frac{1}{2p-p^2} = \frac{p}{2-p}
$$

Para $P(X_1 < X_2)$:
$$
P(X_1 < X_2) = \sum_{k_1=0}^{\infty} P(X_1 = k_1) P(X_2 > k_1)
$$
A probabilidade $P(X_2 > k_1) = (1-p)^{k_1+1}$, então:
$$
P(X_1 < X_2) = \sum_{k_1=0}^{\infty} p(1-p)^{k_1} (1-p)^{k_1+1} = p(1-p) \sum_{k_1=0}^{\infty} (1-p)^{2k_1}
$$
$$
= p(1-p) \cdot \frac{1}{1-(1-p)^2} = \frac{1-p}{2-p}
$$

(b) $X_1|X_1 + X_2 = n \sim UD\{0, 1, \dots, n\}$ (Uniforme Discreta).

Para determinar $P(X_1 = k | X_1 + X_2 = n)$:
$$
P(X_1 = k | X_1 + X_2 = n) = \frac{P(X_1 = k, X_2 = n-k)}{P(X_1 + X_2 = n)}
$$
$$
= \frac{p(1-p)^k \cdot p(1-p)^{n-k}}{(n+1)p^2(1-p)^n} = \frac{1}{n+1}
$$
para $k = 0, 1, \dots, n$.



## Questão 2

Uma certa lâmpada tem uma vida, em horas, seguindo uma distribuição exponencial de parâmetro $\lambda = 1$. Um jogador acende a lâmpada e, enquanto a lâmpada ainda estiver acesa, lança um dado equilibrado de 15 em 15 segundos. Qual o número esperado de 3’s lançados pelo jogador até a lâmpada se apagar?

**Resposta:** Seja $X$ o número total de 3’s lançados pelo jogador até a lâmpada se apagar. Logo, $E(X) = 40$.

A vida da lâmpada $T \sim \exp(1)$. O tempo esperado de vida é $E[T] = 1$ hora. O jogador lança o dado a cada 15 segundos, ou seja, 240 vezes por hora. A chance de sair '3' em cada lançamento é $1/6$. Assim, a taxa de '3s' por hora é $240 \times (1/6) = 40$. O número esperado de '3s' é $E[X] = 40 \cdot E[T] = 40 \cdot 1 = 40$.



## Questão 3

Sejam $X \sim \mathrm{Bin}(m, p)$ e $Y \sim \mathrm{Bin}(n, p)$ variáveis aleatórias independentes. Obtenha a distribuição condicional de $X$ dada $X + Y$. Qual o nome desta distribuição?

**Resposta:** $X|X + Y = r \sim \mathrm{Hgeo}(m, m+n, r)$ (Hipergeométrica).

A distribuição condicional é:
$$
P(X = k | X+Y = r) = \frac{\binom{m}{k} \binom{n}{r-k}}{\binom{m+n}{r}}
$$



## Questão 4

Sejam $X \sim \mathrm{Poisson}(\lambda_1)$ e $Y \sim \mathrm{Poisson}(\lambda_2)$ variáveis aleatórias independentes. Obtenha a distribuição condicional de $X$ dada $X + Y$. Qual o nome desta distribuição?

**Resposta:** $X|X + Y = n \sim \mathrm{Bin}(n, \lambda_1/(\lambda_1+\lambda_2))$ (Binomial).

A distribuição condicional é:
$$
P(X = k | X+Y = n) = \binom{n}{k} \left(\frac{\lambda_1}{\lambda_1+\lambda_2}\right)^k \left(\frac{\lambda_2}{\lambda_1+\lambda_2}\right)^{n-k}
$$
## Questão 5
5. Suponha que o vetor $(X,Y)$ possua distribuição normal bivariada com densidade $f(x, y)$.  
(a) Encontre as densidades marginais de $X$ e $Y$.  
(b) Obtenha a distribuição condicional de $X$ dada $Y$.  
**Resposta:**  
(a) $X \sim N(\mu_1, \sigma_1^2)$ e $Y \sim N(\mu_2, \sigma_2^2)$.  
(b) $X|Y = y \sim N\left(\mu_1 + \rho \frac{\sigma_1}{\sigma_2}(y - \mu_2),\ \sigma_1^2(1- \rho^2)\right)$.  

A densidade conjunta de um vetor bivariado normal é dada por:  
$$
f(x, y) = \frac{1}{2\pi\sigma_1\sigma_2 \sqrt{1- \rho^2}} \exp\left\{ - \frac{1}{2(1- \rho^2)} \left[ \left(\frac{x- \mu_1}{\sigma_1}\right)^2 - 2\rho\left(\frac{x- \mu_1}{\sigma_1}\right)\left(\frac{y - \mu_2}{\sigma_2}\right) + \left(\frac{y - \mu_2}{\sigma_2}\right)^2 \right] \right\}
$$

(a) As densidades marginais de $X$ e $Y$ para uma distribuição normal bivariada são, por si só, distribuições normais:
- $X \sim N(\mu_1, \sigma_1^2)$
- $Y \sim N(\mu_2, \sigma_2^2)$

(b) A distribuição condicional de $X$ dado $Y=y$ para uma distribuição normal bivariada também é normal.  
- Média condicional: $E[X|Y=y] = \mu_1 + \rho \frac{\sigma_1}{\sigma_2}(y-\mu_2)$  
- Variância condicional: $Var(X|Y=y) = \sigma_1^2(1-\rho^2)$  
Portanto, $X|Y=y \sim N\left(\mu_1 + \rho \frac{\sigma_1}{\sigma_2}(y-\mu_2),\ \sigma_1^2(1-\rho^2)\right)$.

## Questão 6
6. Considere o seguinte experimento de 2 etapas: primeiro, escolhe-se um ponto $x$ de acordo com a distribuição uniforme em $(0, 1)$; depois escolhe-se um ponto $y$ de acordo com a distribuição uniforme em $(-x, x)$. Se o vetor $(X,Y)$ representar o resultado do experimento:  
(a) Qual será a densidade conjunta de $X$ e $Y$?  
(b) A densidade marginal de $Y$?  
(c) A densidade condicional de $X$ dada $Y$?  
**Resposta:**  
(a) $f(x, y) = \frac{1}{2x}\ 1\{-x<y<x,\ 0<x<1\}$  
(b) $f_Y(y) = \frac{1}{2} \log\left(\frac{1}{|y|}\right)\ 1\{-1<y<1\}$  
(c) $f_{X|Y}(x|y) = \frac{1}{x \log(1/|y|)}\ 1\{0<x<1,\ -1<y<1\}$

## Questão 7
7. Observam-se duas lâmpadas durante suas vidas úteis. Suponha as vidas independentes e exponenciais de parâmetro $\lambda$. Sejam $X$ o tempo de queima da primeira lâmpada a queimar e $Y$ o tempo de queima da segunda a queimar ($X \le Y$).  
(a) Qual a distribuição condicional de $X$ dada $Y$?  
(b) Qual a distribuição de $Y$ dada $X$?  
**Resposta:**  
(a) $P(X \le x|Y = y) = \begin{cases} 0, & x < 0 \\ \frac{1-e^{-\lambda x}}{1-e^{-\lambda y}}, & 0 \le x < y \\ 1, & x \ge y \end{cases}$  
(b) $P(Y \le y|X = x) = [1- e^{-\lambda(y-x)}]\, 1\{y>x\}$

## Questão 8
8. Sejam $X$ e $Y$ o mínimo e o máximo de duas variáveis aleatórias independentes com distribuição comum $\exp(\lambda)$, $\lambda > 0$. Verifique, de duas maneiras, que $Y-X|X \sim \exp(\lambda)$:  
(a) A partir da densidade conjunta de $X$ e $Y-X$.  
(b) Utilizando o princípio de substituição e o resultado do Exercício 7(b).  
**Resposta:**  
(a) Use o método do Jacobiano.  
(b) Aplicação direta do Exercício 7(b).

## Questão 9
9. Verifique que, se $X$ é constante quase certamente, i.e., $P(X = c) = 1$, então $P(X = c|Y = y) = 1$. Deduza a propriedade: se $X = c$, para alguma constante $c$, então $E(X|Y) = c$.  
**Resposta:**  
Se $X = c$ quase certamente, então $P(X = c|Y = y) = 1$ para todo $y$ tal que $P(Y=y)>0$. Portanto, $E[X|Y] = c$.

## Questão 10
10. Sejam $X$ e $Y$ variáveis aleatórias tais que $E(X^2) < \infty$ e $E(Y^2) < \infty$. Verifique que $\mathrm{Cov}(X,Y) = \mathrm{Cov}(X,E(Y|X))$.  
**Resposta:**  
$\mathrm{Cov}(X,Y) = \mathrm{Cov}(X,E(Y|X))$.

## Questão 11
11. Suponha que $X$ e $Y$ possuam densidade conjunta $f(x, y) = 1/\pi$, se $x^2 + y^2 \le 1$; $0$, caso contrário.  
(a) Obtenha a distribuição condicional de $Y$ dada $X$. Calcule $E(Y|X)$.  
(b) $X$ e $Y$ são independentes? Por quê?  
(c) Verifique que $X$ e $Y$ são não correlacionadas.  
**Resposta:**  
(a) $f_{Y|X}(y|x) = \frac{1}{2\sqrt{1-x^2}}\, 1\{-\sqrt{1-x^2}<y<\sqrt{1-x^2},\ -1<x<1\}$; $E(Y|X) = 0$.  
(b) Não, não são independentes.  
(c) Sim, são não correlacionadas.

## Questão 12
12. Seja $Y$ uma variável aleatória discreta com $P(Y = 1) = 1/8$ e $P(Y = 2) = 7/8$. Se $0 < p < 1$ e $X|Y = \begin{cases} 2Y, & \text{com probabilidade } p \\ 3Y, & \text{com probabilidade } 1-p \end{cases}$, determine $E(X|Y)$.  
**Resposta:**  
$E(X|Y) = (3-p)\,1\{Y=1\} + 2(3-p)\,1\{Y=2\}$

## Questão 13
13. Sejam $X \sim \exp(1)$ e $t > 0$ fixo. Encontre $E(X|\max\{X, t\})$ e $E(X|\min\{X, t\})$.  
**Resposta:**  
Sejam $U = \max\{X, t\}$ e $V = \min\{X, t\}$.  
$E(X|U) = U\,1\{U>t\} + \frac{1-(t+1)e^{-t}}{1-e^{-t}}\,1\{U=t\}$  
$E(X|V) = V\,1\{V<t\} + (1+t)\,1\{V=t\}$

## Questão 14
14. Seja $(X,Y)$ um vetor aleatório bidimensional. Suponha que (i) $X \sim \exp(1/2)$ (ii) para cada $x > 0$, $Y|X = x \sim U[0, x^2]$.  
(a) Qual a distribuição de $Z = Y/X^2$?  
(b) Calcule $E(X)$, $E(Y)$ e $\mathrm{Cov}(X,Y)$.  
**Resposta:**  
(a) $P(Z \le z) = \begin{cases} 0, & z < 0 \\ z, & 0 \le z < 1 \\ 1, & z \ge 1 \end{cases}$ (ou seja, $Z \sim U$).  
(b) $E(X) = 2$, $E(Y) = 4$ e $\mathrm{Cov}(X,Y) = 16$.

## Questão 15
15. Determine $E(X|Y)$ e $E(Y|X)$ no Exercício 6. Calcule $\mathrm{Cov}(X,Y)$. As variáveis $X$ e $Y$ são independentes?  
**Resposta:**  
$E(X|Y) = \frac{1-|Y|}{\log(1/|Y|)}\,1\{-1<Y<1\}$, $E(Y|X) = 0$, $\mathrm{Cov}(X,Y) = 0$. As variáveis $X$ e $Y$ não são independentes.

## Questão 16
16. (a) Verifique que, se $X$ e $Y$ são independentes, então $P(X < Y) = \int_{-\infty}^\infty [1- F_Y(x)]\, dF_X(x)$.  
(b) Sejam $X$ e $Y$ independentes, $X$ exponencial de parâmetro $\lambda$ e $Y$ uniforme em $[0, \lambda]$, $\lambda > 0$. Obtenha $P(X < Y)$, $P(X > Y)$ e $P(X = Y)$.  
**Resposta:**  
(a) Use a Lei da Esperança Total.  
(b) $P(X < Y) = \frac{\lambda^2 + e^{-\lambda^2} - 1}{\lambda^2}$, $P(X > Y) = \frac{1-e^{-\lambda^2}}{\lambda^2}$, $P(X = Y) = 0$.

## Questão 17
17. Seja $f_{X,Y}(x, y) = 1_A(x, y)$, onde $A$ é o triângulo com vértices $(0, 0)$, $(2, 0)$, $(0, 1)$. Encontre a densidade condicional $f_{Y|X}(y|x)$ e a esperança condicional $E(Y|X = 1)$.  
**Resposta:**  
$f_{Y|X}(y|x) = \frac{2}{2-x}\,1\{0<y<(2-x)/2,\ 0<x<2\}$  
$E(Y|X=1) = 1/4$

## Questão 18
18. Seja $f_{X,Y}(x, y) = (x + y)\,1_A(x, y)$, onde $A = [0,1]\times[0,1]$. Encontre $E(X|Y = y)$, para cada $y \in \mathbb{R}$.  
**Resposta:**  
$E(X|Y = y) = \frac{3y+2}{6y+3}\,1\{0<y<1\}$

## Questão 19
19. Seja $F(x, y) = \begin{cases} 1, & x+ 2y > 1 \\ 0, & \text{caso contrário} \end{cases}$. A função $F$ define uma função de distribuição no plano?  
**Resposta:**  
Não! Pois existe $a_1 < b_1$ e $a_2 < b_2$ tal que $F(b_1, b_2) - F(b_1, a_2) - F(a_1, b_2) + F(a_1, a_2) < 0$.

## Questão 20
20. Sejam $Y,Z$ variáveis aleatórias i.i.d, com distribuição exponencial de parâmetro $\lambda > 0$. Denote $V = Y + Z$. Qual a densidade condicional de $Y$ dado $V$? Calcule $E(Y|V)$.  
**Resposta:**  
$E(Y|V) = \frac{V}{2}\,1\{V > 0\}$
## Questão 21

O vetor $(X,Y)$ tem densidade conjunta $f$ dada por $f(x, y) = 1/4$ dentro do quadrado com vértices nos pontos $(1, 0)$, $(0, 1)$, $(-1, 0)$ e $(0,-1)$ no plano, e $f(x, y) = 0$ em outro caso. Encontre as densidades marginais de $X$ e $Y$, as duas densidades condicionais e verifique que $E(X|Y) = E(Y|X)$.

**Resposta:**  
$f_X(x) = \frac{1}{2}\ 1\{-1 < x < 1\}$  
$f_Y(y) = \frac{1}{2}\ 1\{-1 < y < 1\}$  
$f_{X|Y}(x|y) = \frac{1}{2}\ 1\{-1 < x, y < 1\}$  
$f_{Y|X}(y|x) = \frac{1}{2}\ 1\{-1 < x, y < 1\}$

A descrição "quadrado com vértices nos pontos $(1, 0)$, $(0, 1)$, $(-1, 0)$ e $(0,-1)$" define uma região em forma de losango (ou diamante) dada por $|x|+|y| \le 1$. A área dessa região é 2. No entanto, as respostas fornecidas para $f_X(x)$ e $f_Y(y)$ são $1/2$ para $x,y \in (-1,1)$, o que corresponde à densidade marginal de uma distribuição uniforme em um quadrado $[-1,1] \times [-1,1]$ (área 4). As respostas para $f_{X|Y}(x|y)$ e $f_{Y|X}(y|x)$ também implicam um quadrado $[-1,1] \times [-1,1]$ e independência. Seguindo a resposta fornecida no Rpt, que deve ser a interpretação pretendida do problema, assumimos que $(X,Y)$ está uniformemente distribuído no quadrado $[-1,1] \times [-1,1]$. A área deste quadrado é $2 \times 2 = 4$. Então, $f_{X,Y}(x,y) = 1/4$ para $-1 < x < 1,\ -1 < y < 1$.

- **Densidades marginais de $X$ e $Y$:**  
    $f_X(x) = \int_{-1}^1 \frac{1}{4} dy = \frac{1}{2}$ para $-1 < x < 1$.  
    $f_Y(y) = \int_{-1}^1 \frac{1}{4} dx = \frac{1}{2}$ para $-1 < y < 1$.

- **Densidades condicionais de $X$ dado $Y$ e $Y$ dado $X$:**  
    $f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)} = \frac{1/4}{1/2} = \frac{1}{2}$ para $-1 < x < 1,\ -1 < y < 1$.  
    $f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{1/4}{1/2} = \frac{1}{2}$ para $-1 < x < 1,\ -1 < y < 1$.

- **Verificar que $E(X|Y) = E(Y|X)$:**  
    $E[X|Y=y] = \int_{-1}^1 x \cdot f_{X|Y}(x|y) dx = \int_{-1}^1 x \cdot \frac{1}{2} dx = 0$.  
    Portanto, $E[X|Y] = 0$.  
    Por simetria, $E[Y|X=x] = \int_{-1}^1 y \cdot \frac{1}{2} dy = 0$.  
    Portanto, $E[Y|X] = 0$.  
    Conclui-se que $E(X|Y) = E(Y|X)$.

::::